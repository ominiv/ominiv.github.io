---
title: ML/Ensemble GBM
date: 2021-12-29 15:30:09
categories:
- ML
tags:
- GBM
---

# [ML/Ensemble] Gradient Boosting Machine

<img src = "https://drive.google.com/uc?export=download&id=1b9ENW4zjogz7Hlkl06d-t2XGdn0Knj6f" width="600px">



- 부스팅 알고리즘은  약한분류기를 순차적으로 결합하여 강한 분류기를 만드는 방식입니다.
- 부스팅의 대표적인 구현은 Adaptive Boost와  Gradient Boost가 있다.

<br>

-----------

## AdaBoost vs. GBM

AdaBoost와 GBM은 잔차를 보완하는 방법이 다릅니다. <br>AdaBoost는 이전 모델에서 **잘못 분류한 데이터에 가중치**를 더 많이 주고 , **잘 분류한 학습기에 더  많은 가중치**를 주어 해결합니다. <br>AdaBoost의 최종모형에서 알 수 있듯이 $ \alpha_i $로 학습기 마다 가중치를 다르게 주고, 각 데이터에도 잘못 분류되면 가중치가 커지도록 adaptive하게 잔차를 보완합니다.

$$
F(x) = \text{sign} (\sum_{t=1}^{T} \alpha_t f_t(x))
$$

이와 달리 GBM에서는 약한 학습기를 잔차(Residual)에 집중합니다.

<br>

------------------

## GBM

- 경사하강법을 결합한 boosting 알고리즘 <br>*경사하강법(Gradient Descent) 오류값을 최소화 하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 방법*
- 순차적인 예측 오류 보정을 통해 학습을 수행하므로 병렬처리가 불가능함
- LightGBM, CatBoost, XGBoost와 같은 파이썬 패키지 존재함

| 장점                             | 단점        |
| -------------------------------- | ----------- |
| ML 계열 모델 중 성능이 좋은 편임 | 긴 수행시간 |

<br>

----------------

## GBM 원리

Gradient Boosting Machine의 작동원리에 대해 알아보자.<br>먼저 첫번째 약한 모델 $f_1(x)$ 를 만듭니다. 다음으로 정답과 예측값의 차이, 즉 잔차 $R_1$ 을 계산합니다.

$$
y = f_1(x) + R_1\\
R_1 = y -f_1(x)
$$

다음 $R_1$을 예측하는 두번째 약한모델 $f_2(x)$를 만듭니다.  그리고 $R_1$과$f_2(x)$의 잔차를 구합니다.

$$
R_1 =f_2(x) + R_2\\R_2 = R_1-f_2(x)
$$

이렇게 $t$번째 약한 모델 $f_t(x)$는 $t-1$번째 잔차에 근사하는 함수입니다. 이를 정리하면 다음과 같은 식이 됩니다.  <br>첫 잔차는 크지만 근사하는 모델을 더해 나가면 마지막 잔차 $R_t$는 매우 작아집니다.  

$$
y= f_1(x) + f_2(x) + ... +f_t(x)+R_t
$$

이러한 반복을 하면 잔차는 계속 줄어들고 training set을 잘 설명하는 예측 모형을 만들 수 있습니다. <br>하지만 이러한 방식은 bias를 줄일수는 있어도 과적합이 일어날 수 있는 단점이 있습니다. <br>실제로 GBM을 이용할 때는 sampling , penelizing 등 regularization 을 이용해 advanced model을 이용하는것이 보편적이다. 

<Br>

✔ wiki gradient boosting algorithm을 살펴보자.

- input 으로 dataset과 미분가능한 loss function를 정의한다. (GBM loss function으로는 주로 MSE, L1 loss, Logistic loss가 사용된다.) 
  - Step1 : $F_0(x)$로 초기 예측값을 준다.
  - Step2 : 아래 과정을 반복하여 최종 예측값을 출력한다.
    - Step2-1 : loss function을 미분해서 잔차를 구한다. 
    - Step2-2 : `2-1`에서 구한  residual을 target으로 하는 week learner(tree model)를 만들고 fitting시킨다.
    - Step2-3 : loss를 최소로 하는 tree model 결과값을 출력한다.
    - Step2-4 : 잔차 예측값 * learning rate 를 이전 예측값과 더해 새로운 예측값을 업데이트 한다.

<br>

--------

##  Parameter

scikit learn은 GradientBoostingClassifier, GradientBoostingRegressor 클래스를 제공함

```python
class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)

class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)
```

https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier

<br>

| parameter                   | description                                                  |
| --------------------------- | ------------------------------------------------------------ |
| loss                        | 경사하강법에서 사용할 loss function                          |
| learning_rate:  default=0.1 | GBM이 학습을 진행할때마다 적용하는 학습률<Br>너무 작은 값을 적용하면 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 있음. 하지만 수행시간이 오래걸리고  너무 작게 설정할경우 최소 오류값에 도달하지 못하는 경우가 존재함 <br>반대로 큰 값을 적용하면 최소오류값을 지나쳐 버려 예측성능이 떨어질 가능성이 높음 (수행속도 빠름) |
| n_estimators : default=100  | weak learner의 개수<br>weak learner는 순차적으로 오류를 보정하므로 개수가 많을 수록 예측 성능이 좋아짐 |
| subsample : default=1.0     | weak learner가 학습에 사용하는 데이터의 샘플링 비율. <br>default=1으로 전체 학습데이터를 기반으로 학습한다는 의미다. 간혹 과적합이 염려되는경우 1보다 작은 값으로 선정하기도 함 |

<br>

--------------------

## Practice

- UCI Machine learning Repository 에서 제공하는 Human Acitivity Recognition Dataset 활용 [link](https://github.com/ominiv/Practice_ML/blob/master/UCI_Human_activity_dataset.ipynb)
- Practic_GBM [link](https://github.com/ominiv/Practice_ML/blob/master/Practice/Practice%20GBM.ipynb)

<br>

-----

## Reference

- [assaeunji]님의 GBM (Gradient Boosting Machines)에 대한 자세한 설명 (1): Regression  [link](https://assaeunji.github.io/machine%20learning/2020-09-05-gbm/)
- [hyunlee103] 님의 [머신러닝] Boosting Algorithm  [link](https://hyunlee103.tistory.com/25)
- [Deep Play] 님의 Gradient Boosting Algorithm의 직관적인 이해 [link](https://3months.tistory.com/368)

