---
title: ML/FeatureExtraction PCA
date: 2022-01-13 00:00:01
categories:

- ML
tags:
- FeatureExtraction
- PCA

---

# [ML/FeatureExtraction] PCA(Principal Component Analysis)
PCA가장 대표적인 차원축소 기법이다. <br>여러변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법이다.<br>**PCA로 차원을 축소할 때는 기존 데이터의 정보유실이 최소화 되는 것이 당연하다. 이를 위해 가장 높은 분산을 가지는 데이터 축을 찾아 이 축으로 차원을 축소하는데 이것이 PCA의 주성분이 된다.**

PCA는 제일 먼저 가장 큰 데이터 변동성(Variance)을 기반으로 첫번째 벡터 축을 생성하고, 두번째 축은 첫번째 벡터 축에 직각이 되는 벡터를 축으로한다. 세번째 축은 다시 두번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성한다.<Br>이렇게 생선된 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수 만큼 차원으로 원붠데이터가 차원 축소된다.

✔ **즉, 주성분 분석은 원본데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수있는 분석법이다.**

---
## Details
PCA를 선형대수 관점으로 해석해보자. 

입력데이터의 공분산 행렬(Covariance Matrix)을 고유값으로 분해하고 이렇게 구한 고유벡터에 입력데이터를 선형변환하는 것이다. 이 고유벡터가 PCA의 주성분벡터로서 입력데이터 분산이 큰 방향을 나타낸다. **고유값(eigenvalue)은 바로 고유벡터의 크기를 나타내고 동시에 입력데이터의 분산을 나타낸다.**  더 자세히 알아보기 위해 선형변환, 공분산행렬과 고유벡터에 대해 알아보자.

### 선형변환
**일반적으로 선형변환은 특정 벡터에 행렬A를 곱해 새로운 벡터를 변환하는 것이다.** <br>이를 특정 벡터를 하나의 공간에서 다른 공간으로 투영하는 개념으로도 볼수 있으며 이경우 이 행렬을 바로 **공간**으로 가정하는 것이다.


### 고유벡터
고유벡터는 행렬A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터를 지칭한다. <br>즉, $Ax= ax (A : matrix, x = eigen vector, a = scalar)$ <br>정방행렬은 최대 그 차원수 만큼의 고유벡터를 가질 수 있다. 예를 들어 2x2 행렬은 2개의 고유벡테를 가질 수 있다. 

**이렇게 고유벡터는 행렬이 작용하는 힘과 방향과 관계가 있기에 행렬을 분해하는 데 사용한다.**


### 공분산 행렬
보통 분산은 한개의 특정한 변수의 데이터 변동을 의미한다. <br>하지만 공분산은 두 변수간의 변동을 의미한다. 예를들어 $Cov(키,몸무게) > 0$ 이라고 한다면 키가 증가할때 몸무게도 증가 한다는 것이다.

공분산 행렬은 여러 변수와 공분산을 포함하는 정방형(Diagonal)행렬이며 대칭(Symmetric)행렬이다.<br> 즉, $A^T = A$인 행렬이 대칭행렬이다. 

| |x|y|z|
|--|--|--|--|
|x|**3.0**|-0.7|-0.2|
|y|-0.7|**4**|0.2|
|z|-0.2|0.2|**0.9**|

위 표를 보면 공분산 행렬에서 대각원소는 각 x,y,z의 분산이고 대각선 이외에 원소는 변수 간의 공분산을 의미한다. 
- x,y,z 분산 = [3, 4, 0.9]
- $Cov(x,y)=-0.7, Cov(x,z)=-0.2, Cov(z,y)=0.2$

대칭행렬은 고유값 분해와 관혈해 매우 좋은 특성이 있다. <br>**대칭행렬은 항상 고유벡터를 직교행렬로 고유값을 정방행렬로 대각화 할 수 있다는 것이다.**<Br>입력데이터의 공분한 행렬을 $C$라고하면 공분산 행렬의 특성으로 아래와 같이 분해가 된다. 

$$
C = P \sum P^T ,(P :n \times n 직교행렬, \sum: n \times n 정방행렬)
$$

$$
C = \begin{bmatrix}e_1& \dots &e_4 
\end{bmatrix}\begin{bmatrix}\lambda_1& \dots &0\\
\dots& \dots & \dots \\
0& \dots &\lambda_n \\ \end{bmatrix}
\begin{bmatrix}e_1^t\\\dots\\e_n^t \end{bmatrix}
$$

$e_i$는 $i$번째 고유벡터, $\lambda_i$ $i$번째 고유벡터의 크기를 의미한다. <Br> $e_1$는 가장 분산이 큰 방향을 가진 고유벡터이며 $e_2$는 $e_1$에 수직이면서 다음으로 가장 분산이 큰방향을 가진 고유벡터다.<Br>**입력데이터의 공분산행렬이 고유벡터와 고유값으로 분해가 될수 있으며 이렇게 ㄷ분해된 고유벡터를 이용해 입력데이터를 선형변환하는 방식이 PCA다.**


## PCA Process
1. 입력데이터세트의 공분산 행렬을 생성
2. 공분산 행렬의 고유벡터와 고유값을 계산
3. 공분산이 큰 순서대로 K개 만큼 고유벡터를 추출
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환

---

## Tutorial
PCA를 진행할때 사용되는 값들의 scale을 맞춰주는게 좋다. 각 열에대해 scaling을 수행한다<br> 당연하게도 원본데이터 세트 대비 PCA 변환데이터세트의 예측성능은 떨어질수 밖에 없다. 속성이 줄어들면서 예측성능의 정확도가 원본 데이터 대비 하락할 수 있는점을 기억하자.

```python
from sklearn.decomposition import PCA
pca= PCA(n_components=2)
pca.fit(scaled_data)
iris_pca= pca.transform(scaled_data)
```
<img src="https://drive.google.com/uc?export=download&id=1dsL9rOtJ1dKbMdFttnwD0iecs1EINmk9" width = 300>

위 예제는 PC1 : 72 % , PC2: 22%으로 두개 요소로만 변환해도 원본데이터의 변동성을 95%를 설명할 수 있다. 

---

##  Practice

- Feature extraction PCA [link](https://github.com/ominiv/Practice_ML/blob/master/Practice/Feature%20extraction.ipynb)

-----

## Reference

- 파이썬 머신러닝 완벽가이드 서적
